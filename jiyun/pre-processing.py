import numpy as np
import obspy
from scipy import signal
import warnings
warnings.filterwarnings('ignore')

print("Conventional preprocessing starts!")
print("="*60)

print("=== 6-steps of Conventional Seismic Wave Data Preprocessing ===")
print("Demultiplexing â†’ Trace Editing â†’ Gain Recovery â†’ Filtering â†’ Deconvolution â†’ CMP Gather")

# Data loading
print("\nRaw data loading...")
stream = obspy.read("ANMO_sample.mseed")
print(f"âœ… Loading complete: {len(stream)} tracees")

# Backup original data
original_stream = stream.copy()


print("\n=== 3-channel data visualization ===")

time_axis = {}
# Time and data range by BH1, BH2, BHZ channels
for i, trace in enumerate(stream):
    sampling_rate = trace.stats.sampling_rate
    num_samples = len(trace.data)
    duration = num_samples / sampling_rate
    time_axis[trace.stats.channel] = np.linspace(0, duration, num_samples)
    
    print(f"{trace.stats.channel} Channel:")
    print(f"  Time range: 0 ~ {duration:.1f} seconds")
    print(f"  Data Range: {trace.data.min():.1f} ~ {trace.data.max():.1f}")

print("\nVisualization (using matplotlib):")
import matplotlib.pyplot as plt

fig, axes = plt.subplots(3, 1, figsize=(12, 8))
for i, trace in enumerate(stream):
    channel = trace.stats.channel
    time = time_axis[channel]
    axes[i].plot(time, trace.data)
    axes[i].set_title(f'{channel} Channel')
    axes[i].set_ylabel('Amplitude')
    if i == 2:
        axes[i].set_xlabel('second (s)')
plt.tight_layout()
plt.show()


# =====================================================
# 1ë‹¨ê³„: Demultiplexing (ì—­ë‹¤ì¤‘í™”)
# ì´ë¯¸ ë°ì´í„°ê°€ 3ê°œì˜ íŠ¸ë ˆì´ìŠ¤ì—ì„œ BH1, BH2, BHZë¡œ ë¶„ë¦¬ë˜ì–´ìˆê¸° ë•Œë¬¸ì— í™•ì¸í•˜ê³  ì •ë¦¬í•˜ëŠ” ë‹¨ê³„ë¡œ ì—¬ê¸°ë©´ ëœë‹¤
# =====================================================
print("\nğŸ”¸ Step1: Demultiplexing")
print("- Seperate multi-channel seismic data into individual channels")

demux_channels = {}
for i, trace in enumerate(stream):
    channel_id = trace.stats.channel # ì•ì˜ ì‹œê°í™” ì½”ë“œì—ì„œ channel ê°€ì ¸ì˜¤ê¸°
    # demux_channel dictionary
    demux_channels[channel_id] = {
        'trace': trace,
        'network': trace.stats.network,
        'station': trace.stats.station,
        'channel': trace.stats.channel,
        'sampling_rate': trace.stats.sampling_rate,
        'npts': trace.stats.npts,
        'starttime': trace.stats.starttime
    }
    print(f"  ğŸ“Š {channel_id}: {trace.stats.sampling_rate}Hz, {trace.stats.npts} samples")

print(f"âœ… Step1 complete: {len(demux_channels)} channels seperated")


# =====================================================
# 2ë‹¨ê³„: Trace Editing (íŠ¸ë ˆì´ìŠ¤ í¸ì§‘)
# =====================================================
print("\n   Step2: Trace Editing")
print("- Poor data removal and quality check")

edited_channels = {}
# demux_channelsì˜ itemì„ ê°€ì ¸ì™€ì„œ trace editing ì§„í–‰
for channel_id, ch_data in demux_channels.items():
    trace = ch_data['trace'] # ch_data ë”•ì…”ë„ˆë¦¬ì—ì„œ Obspy ê°ì²´ì¸ 'trace' itemì„ ì¶”ì¶œí•œë‹¤.
    data = trace.data.copy() # Obspy trace ê°ì²´ì˜ trace.dataì˜ ìˆ«ìë§Œ ë³µì‚¬í•˜ì—¬ ì¶”ì¶œí•œë‹¤
    
    print(f"  ğŸ” {channel_id} Quality check...")
    
    # Dead trace ê²€ì‚¬
    if np.all(data == 0) or np.var(data) < 1e-12:
        print(f"    âŒ Dead trace - Remove")
        continue
    
    # NaN/Inf ê°’ ê²€ì‚¬
    if np.any(np.isnan(data)) or np.any(np.isinf(data)):
        print(f"    âš ï¸ NaN/Inf values detected - Correction")
        # Data Cleaning: NaNì„ 0ìœ¼ë¡œ, Infë¥¼ í´ë¦¬í•‘
        data = np.nan_to_num(data, 
                             nan=0.0, # NaNì„ 0ìœ¼ë¡œ
                             posinf=np.max(data[np.isfinite(data)]), # +âˆë¥¼ maxë¡œ
                             neginf=np.min(data[np.isfinite(data)])) # -âˆë¥¼ minìœ¼ë¡œ
    
    # ìŠ¤íŒŒì´í¬ ì œê±° (Z-score > 5)
    # z_scores = (ê°’-í‰ê· ) / í‘œì¤€í¸ì°¨
    z_scores = np.abs((data - np.mean(data)) / (np.std(data) + 1e-10))
    # z_scoresê°€ 5 ì´ìƒì¸ ê°’ì„ spikeë¡œ count (í‘œì¤€í¸ì°¨ì˜ 5ë°° ì´ìƒ ë²—ì–´ë‚œ ê°’ì„ ì˜ë¯¸)
    spike_count = np.sum(z_scores > 5)
    if spike_count > 0:
        print(f"    âš ï¸ {spike_count} Spikes Remove")
        # ìœ„ì¹˜ ì°¾ê¸°: z_scores > 5ê°€ Trueì¸ ê°’ì´ spike_maskì´ë‹¤
        spike_mask = z_scores > 5
        # ìŠ¤íŒŒì´í¬ë¥¼ ì£¼ë³€ ê°’ì˜ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
        for idx in np.where(spike_mask)[0]:
            if idx > 0 and idx < len(data) - 1:
                data[idx] = (data[idx-1] + data[idx+1]) / 2
                
    # í¬í™” ê²€ì‚¬ ì¶”ê°€
    max_val = np.max(np.abs(data)) # ìµœëŒ€ê°’
    saturation_threshold = max_val * 0.95 # í¬í™” ì„ê³„ê°’
    saturated_count = np.sum(np.abs(data) >= saturation_threshold)
    saturation_ratio = saturated_count / len(data)
    
    if saturation_ratio > 0.05:  # Saturation above 5%
        print(f"    âš ï¸ Saturation detected: {saturation_ratio:.1%} ({saturated_count} points)")
        print(f"    ğŸ’¡ ì´ ì±„ë„ì€ ì •ë³´ ì†ì‹¤ì´ ìˆì„ ìˆ˜ ìˆìŒ")
        # í¬í™” ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì €ì¥
        ch_data['quality_flags'] = ['saturation_detected']

    # í¸ì§‘ëœ ë°ì´í„° ì €ì¥
    edited_trace = trace.copy()
    edited_trace.data = data
    edited_channels[channel_id] = edited_trace
    print(f"       Data Quality Check Complete")

print(f"   Step2 complete: {len(edited_channels)} channels")


# =====================================================
# 3ë‹¨ê³„: Gain Recovery (ì´ë“ ë³µêµ¬)
#- ê¸°ë¡ ì‹œ ì ìš©ëœ ì´ë“ì„ ë³´ìƒí•˜ì—¬ ì›ë˜ ì§„í­ ë³µì›
# =====================================================
print("\n   Step3: Gain Recovery")
print("Restore original amplitude by compensating for gain applied during recording")

gain_recovered = {}
# edited_channelsì—ì„œ itemsë¥¼ ê°€ì ¸ì™€ gain recovery ì§„í–‰
for channel_id, trace in edited_channels.items():
    data = trace.data.copy()
    
    print(f"  âš¡ {channel_id} Gain Recovery...")
    
    # ê³„ê¸° ì‘ë‹µ ì œê±° (ê°„ë‹¨í•œ ê³ ì—­í†µê³¼)
    # ë§¤ìš° ë‚®ì€ ì£¼íŒŒìˆ˜ ì„±ë¶„ ì œê±° (0.01Hz ì´í•˜)
    if trace.stats.sampling_rate > 0.02:  # ë‚˜ì´í€´ìŠ¤íŠ¸ ì¡°ê±´
        # ìˆ˜ë™ìœ¼ë¡œ ê³ ì—­í†µê³¼ í•„í„° êµ¬í˜„ (scipy ì‚¬ìš©)
        nyquist = trace.stats.sampling_rate / 2
        low_cutoff = min(0.01, nyquist * 0.01)  # safe cutoff
        
        try:
            b, a = signal.butter(2, low_cutoff / nyquist, btype='high')
            final_data = signal.filtfilt(b, a, data)
            print(f"        HPF applied (cutoff: {low_cutoff:.4f}Hz)")
        except:
            final_data = data
            print(f"    âš ï¸ HPF Failed - Skip")
    else:
        final_data = data
        print(f"    âš ï¸ Sampling frequency too low - Skip")
    # ê²°ê³¼ ì €ì¥
    recovered_trace = trace.copy()
    recovered_trace.data = final_data
    gain_recovered[channel_id] = recovered_trace
    
    print(f"       Gain Recovery complete")

print(f"   Step 3 complete: {len(gain_recovered)} channels gain recovered")


def simple_trend_check(data, sampling_rate):
    """ê°„ë‹¨í•œ íŠ¸ë Œë“œ í™•ì¸"""

    # ì‹œê°„ì¶• ìƒì„±
    time = np.arange(len(data)) / sampling_rate

    # ì‹œê°í™”
    plt.figure(figsize=(12, 6))

    plt.subplot(2, 1, 1)
    plt.plot(time, data)
    plt.title('raw data')
    plt.ylabel('Amplitude')

    # ê°„ë‹¨í•œ ì´ë™í‰ê· ìœ¼ë¡œ íŠ¸ë Œë“œ í™•ì¸
    window_size = len(data) // 20  # ì „ì²´ì˜ 5%
    moving_avg = np.convolve(data, np.ones(window_size)/window_size, mode='same')

    plt.subplot(2, 1, 2)
    plt.plot(time, moving_avg, 'r-', linewidth=2)
    plt.title('moving average (approximate trend)')
    plt.xlabel('second (s)')
    plt.ylabel('Amplitude')

    plt.tight_layout()
    plt.show()

    # ê°„ë‹¨í•œ ê¸°ì¤€
    trend_range = np.max(moving_avg) - np.min(moving_avg)
    data_range = np.max(data) - np.min(data)
    trend_ratio = trend_range / data_range

    print(f"trend ratio: {trend_ratio:.2%}")

    return trend_ratio
    '''
    if trend_ratio > 0.3:  # 30% ì´ìƒì´ë©´
        return "íŠ¸ë Œë“œ ì˜ì‹¬ - ì œê±° ê³ ë ¤"
    else:
        return "íŠ¸ë Œë“œ ë¯¸ë¯¸ - ì œê±° ë¶ˆí•„ìš”"
    '''


# =====================================================
# 4ë‹¨ê³„: Filtering (í•„í„°ë§)
# =====================================================
print("\n    Step4: Filtering")
print("- Noise Removal in the frequency domain")

filtered_channels = {}
# gain_recoveredì˜ itemsë¥¼ ê°€ì ¸ì™€ filteringì„ ìˆ˜í–‰í•œë‹¤
for channel_id, trace in gain_recovered.items():
    data = trace.data.copy()
    
    print(f"  ğŸ›ï¸ {channel_id} Filtering...")
    
    # 1. Linear Trend Removal
    x = np.arange(len(data))
    if simple_trend_check(data, sampling_rate) > 0.3:
        slope = np.sum((x - np.mean(x)) * (data - np.mean(data))) / np.sum((x - np.mean(x))**2)
        intercept = np.mean(data) - slope * np.mean(x)
        trend = slope * x + intercept
        detrended_data = data - trend
        """"
        1) ìµœì†Œì œê³±ë²•(Least Squares)
           slope = Î£[(x-xÌ„)(y-È³)] / Î£[(x-xÌ„)Â²]
           xÌ„ = np.mean(x)     # xì˜ í‰ê· 
           È³ = np.mean(data)   # yì˜ í‰ê· 
        2) intercept
           ì§ì„ ì˜ ë°©ì •ì‹: y = slope * x + intercept
           intercept = È³ - slope * xÌ„
        3) íŠ¸ë Œë“œ ë¼ì¸ ìƒì„±
        4) ë°ì´í„°ì—ì„œ íŠ¸ë Œë“œ ì œê±°
        """
        print(f"trend removed!\n")
    else:
        detrended_data = data
        print(f"trend not removed!(stabilized)\n")
    
    # 2. ë°´ë“œíŒ¨ìŠ¤ í•„í„° (1-20Hz) - directly using scipy
    # why 1-20Hz?: Practically optimized range of most useful seismic wave
    sampling_rate = trace.stats.sampling_rate   # 40 Hz here
    nyquist = sampling_rate / 2   # 20Hz here

    # ì•ˆì „í•œ ì£¼íŒŒìˆ˜ ë²”ìœ„ ì„¤ì •
    low_freq = min(1.0, nyquist * 0.1) # 1 or 10% of nyquist
    high_freq = min(20.0, nyquist * 0.9) # 20 or 90% of nyquist
    
    if low_freq < high_freq and nyquist > low_freq: # the latter: checking if filtering is available
        try:
            # ë²„í„°ì›ŒìŠ¤ ë°´ë“œíŒ¨ìŠ¤ í•„í„°
            b, a = signal.butter(4, [low_freq/nyquist, high_freq/nyquist], btype='band')
            # butter(order, normalized frequency, ...)
            # butterworth BPF uses normalized frequency
            bandpass_data = signal.filtfilt(b, a, detrended_data)
            # filtfilt: two-way filtering (to compensate phase delay)
            print(f"    -> BPF applied: {low_freq:.1f}-{high_freq:.1f}Hz")  # It means, BPF range is 1.0-18.0Hz
        except Exception as e:
            print(f"    -> BPF failed: {e}")
            bandpass_data = detrended_data  # fallback to the former detrended_data
    else:
        print(f"       inappropriate frequency range- skip filtering") # not applying filtering
        bandpass_data = detrended_data
    
    # 3. ë…¸ì¹˜ í•„í„° (60Hz ì „ë ¥ì„  ê°„ì„­ ì œê±°)
    if sampling_rate > 120:  # Nyquist condition: need at least 1120Hz to filter 60Hz
        try:
            # Infinite Impulse Response (IIR) Notch filter design
            notch_freq = 60.0
            Q = 30
            b, a = signal.iirnotch(notch_freq, Q, sampling_rate)
            notched_data = signal.filtfilt(b, a, bandpass_data)
            print(f"    -> Notch Filter Applied: {notch_freq}Hz")
        except Exception as e:
            print(f"    -> Nothch Filter Failed: {e}")
            notched_data = bandpass_data
    else:
        notched_data = bandpass_data
    
    # ê²°ê³¼ ì €ì¥
    filtered_trace = trace.copy()
    filtered_trace.data = notched_data
    filtered_channels[channel_id] = filtered_trace

print(f"  Step4 completed: Filtering {len(filtered_channels)} channels")


# =====================================================
# 5ë‹¨ê³„: Deconvolution (ì—­ì»¨ë³¼ë£¨ì…˜)
#- ì§€ì§„íŒŒ ì „íŒŒ ê³¼ì •ì—ì„œ ë°œìƒí•œ íŒŒí˜• ì™œê³¡ ë³´ì •
# =====================================================
print("\n   Step5: Deconvolution")
print("Correction of waveform distortion caused by seismic wave propagation")
 
deconvolved_channels = {}
# í•„í„° ì ìš©í•œ filtered_channelsì˜ itemsë¥¼ ê°€ì ¸ì˜¨ë‹¤
for channel_id, trace in filtered_channels.items():
    data = trace.data.copy()
    
    print(f"  ğŸ”„ {channel_id} Deconvolution...")
    
    # 1. Predictive Deconvolution (simple form)
    # Purpose?: Remove reverberation / Pulse compression / Improve signal / Improve noise characteristic

    # Autocorrelation based Predictive filter
    autocorr_length = min(100, len(data) // 10)  # safe length
    
    if len(data) > autocorr_length * 2:
        # Calculate Autocorrelation
        autocorr = np.correlate(data, data, mode='full')
        # autocorrelation: correlate by oneself / mode: decide output size- full(max info))
        center = len(autocorr) // 2     # finding center
        autocorr = autocorr[center:center + autocorr_length]    # extract only positive lag
        
        # Prediction Filter design
        if len(autocorr) > 1 and np.std(autocorr) > 0:
            # Standardize pattern 
            # autocorr[0]: Total energy of the signal
            # effect: ì‹ í˜¸ì˜ í¬ê¸° ì •ë³´ëŠ” ì—†ì–´ì§€ì§€ë§Œ, ì‹ í˜¸ì˜ íŒ¨í„´ì„ ì•Œ ìˆ˜ ìˆë‹¤.
            autocorr = autocorr / autocorr[0]
            
            # 2-order prediction filter
            if len(autocorr) >= 3: # at least 3 to accesss autocorr[0], [1], [2]
                # when the predictive deconvolution is applied, the equation looks like below
                #   e[n]   =   x[n]   -   autocorr[1]*x[n-1]   +   (autocorr[2]/2) * x[n-2]
                pred_filter = np.array([1, -autocorr[1], autocorr[2]/2])
            else:
                # when the predictive deconvolution is applied, the equation looks like below
                #   e[n]   =   x[n]   -   0.5 * x[n-1]
                pred_filter = np.array([1, -0.5])
        else:
            # when the predictive deconvolution is applied, the equation looks like below
            #   e[n]   =   x[n]
            pred_filter = np.array([1])
        
        # Apply predictive deconvolution 
        try:
            deconv_data = signal.lfilter(pred_filter, [1], data)
        except:
            deconv_data = data
    else:
        deconv_data = data
    
    # 2. Spiking deconvolution effect (enhance High Frequency (HF) components)
    # : to enhance seismic wave resolution, we enhance HF components
    # Enhance HF by 1-order differentiation
    diff_data = np.diff(deconv_data, prepend=deconv_data[0])
    
    # Weighted sum of original and derivative (70% original + 30% HF enhancement)
    enhanced_data = 0.7 * deconv_data + 0.3 * diff_data
    
    # Save results
    deconv_trace = trace.copy()
    deconv_trace.data = enhanced_data
    deconvolved_channels[channel_id] = deconv_trace
    
    print(f"       Deconvolution Complete")

print(f"   Step5 Complete: Deconvolution of {len(deconvolved_channels)} channels")


# =====================================================
# 6ë‹¨ê³„: CMP Gather (ê³µí†µ ì¤‘ì  ì§‘í•©)
#- ê°™ì€ ì§€í•˜ ì ì„ ë°˜ì‚¬í•œ ì‹ í˜¸ë“¤ì„ ê·¸ë£¹í™”
# =====================================================
print("\n   Step6: CMP Gather")
print("- Group signals that reflect the same underground point")

# Grouping by channels (Z, N, E components)
gathered_channels = {}
channel_groups = {
    'vertical': [],    # Z component
    'horizontal_1': [], # N, 1 component 
    'horizontal_2': []  # E, 2 component
}

# Take items of deconvolved_channels
for channel_id, trace in deconvolved_channels.items():
    if 'Z' in channel_id: # BHZ
        channel_groups['vertical'].append((channel_id, trace))
    elif 'N' in channel_id or '1' in channel_id: #BH1
        channel_groups['horizontal_1'].append((channel_id, trace))
    elif 'E' in channel_id or '2' in channel_id: #BH2
        channel_groups['horizontal_2'].append((channel_id, trace))

# Representative channel selection for each group
for group_name, traces in channel_groups.items():
    if traces:
        if len(traces) == 1:
            # Single channel
            channel_id, trace = traces[0]
            gathered_channels[f"{group_name}_{channel_id}"] = trace
            print(f"  -> {group_name}: Select {channel_id}")
        else:
            # Multi channel: Select the first one
            channel_id, trace = traces[0]
            gathered_channels[f"{group_name}_{channel_id}"] = trace
            print(f"  -> {group_name}: Select {channel_id} among ({len(traces)} traces)")

print(f"   Step6 Completed: Form {len(gathered_channels)} groups")


# =====================================================
# Final results
# =====================================================
print("\nCompleted 6-steps conventional seismic preprocessing")
print("="*50)
print("Processing results: ")
for step, count in [
    ("1. Demultiplexing", len(demux_channels)),
    ("2. Trace Editing", len(edited_channels)), 
    ("3. Gain Recovery", len(gain_recovered)),
    ("4. Filtering", len(filtered_channels)),
    ("5. Deconvolution", len(deconvolved_channels)),
    ("6. CMP Gather", len(gathered_channels))
]:
    print(f"  {step}: {count} channels")

print("\nFinal output:")
final_processed_data = {}
for channel_name, trace in gathered_channels.items():
    final_processed_data[channel_name] = {
        'data': trace.data,
        'sampling_rate': trace.stats.sampling_rate,
        'channel': trace.stats.channel,
        'length': len(trace.data)
    }
    print(f"  ğŸ”¸ {channel_name}: {len(trace.data)} samples @ {trace.stats.sampling_rate}Hz")

print("\nâœ… Completed conventional preprocessing. Now proceed Deeplearning preprocessing step.")




import numpy as np
import pandas as pd
from datetime import datetime, timedelta

print("Deeplearning preprocessing starts!")
print("="*60)

# ============================================================================
# Step1. Data preparation and validation
# ============================================================================
print("\n   Step1: Data preparation and validation")

# Check conventionally preprocessed seismic wave data
print("Preprocessed seismic wave data:")
for channel_name, data_info in final_processed_data.items():
    print(f"  ğŸ”¸ {channel_name}: {data_info['length']} samples @ {data_info['sampling_rate']}Hz")

# ìœ„ë„/ê²½ë„ íŒŒì‹± í•¨ìˆ˜
def parse_coordinate(coord_str):
    """ìœ„ë„/ê²½ë„ ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜"""
    try:
        if pd.isna(coord_str) or coord_str == '':
            return None
            
        coord_str = str(coord_str).strip()
        
        # ìˆ«ìì™€ ë°©í–¥ ë¶„ë¦¬
        import re
        match = re.match(r'([0-9.]+)\s*([NSEW])', coord_str)
        
        if match:
            value = float(match.group(1))
            direction = match.group(2).upper()
            
            # ë‚¨ìª½(S)ê³¼ ì„œìª½(W)ì€ ìŒìˆ˜
            if direction in ['S', 'W']:
                value = -value
                
            return value
        else:
            # ìˆœìˆ˜ ìˆ«ìì¸ ê²½ìš°
            try:
                return float(coord_str)
            except:
                return None
    except Exception as e:
        print(f"    âš ï¸ ì¢Œí‘œ íŒŒì‹± ì˜¤ë¥˜: {coord_str} -> {str(e)}")
        return None
    
# CSV íŒŒì¼ êµ¬ì¡° ë¬¸ì œ í•´ê²°
print("ğŸ”§ CSV ì¹´íƒˆë¡œê·¸ ë¡œë”© ë° ì •ë¦¬...")

try:
    # ì›ë³¸ CSV íŒŒì¼ ë¡œë”©
    raw_catalog = pd.read_csv("outCountryEarthquakeList_2000-01-01_2025-07-04.csv")
    
    print(f"ğŸ“‹ ì›ë³¸ CSV êµ¬ì¡° í™•ì¸:")
    print(f"  -> ì´ í–‰ìˆ˜: {len(raw_catalog)}")
    print(f"  -> ì»¬ëŸ¼ë“¤: {list(raw_catalog.columns)}")
    
    # ì²« 5í–‰ ë‚´ìš© í™•ì¸
    print("ğŸ“Š ì²« 5í–‰ ë‚´ìš©:")
    for i in range(min(5, len(raw_catalog))):
        print(f"  í–‰ {i}: {raw_catalog.iloc[i, 0]}")
    
    # ì‹¤ì œ ë°ì´í„°ê°€ ì‹œì‘í•˜ëŠ” í–‰ ì°¾ê¸°
    data_start_row = None
    for i in range(len(raw_catalog)):
        first_col = str(raw_catalog.iloc[i, 0])
        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” í–‰ ì°¾ê¸° (ì‹¤ì œ ë°ì´í„°)
        if first_col.isdigit():
            data_start_row = i
            break
    
    if data_start_row is not None:
        print(f"âœ… ì‹¤ì œ ë°ì´í„° ì‹œì‘ í–‰: {data_start_row}")
        
        # í—¤ë”ë¥¼ data_start_row-1ë¡œ, ë°ì´í„°ë¥¼ data_start_rowë¶€í„°
        if data_start_row > 0:
            catalog_df = pd.read_csv("outCountryEarthquakeList_2000-01-01_2025-07-04.csv", 
                                   header=data_start_row-1, 
                                   skiprows=range(0, data_start_row-1))
        else:
            catalog_df = raw_catalog
    else:
        # í—¤ë”ê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •
        print("âš ï¸ ë°ì´í„° ì‹œì‘ í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - ìˆ˜ë™ ì²˜ë¦¬")
        catalog_df = raw_catalog.iloc[2:].copy()  # 3í–‰ë¶€í„° ë°ì´í„°
        
        # ì»¬ëŸ¼ëª… ìˆ˜ë™ ì„¤ì •
        expected_columns = ['number', 'magnitude', 'depth', 
                           'latitude', 'longitude', 'location']
        catalog_df.columns = expected_columns[:len(catalog_df.columns)]
    
    print(f"ğŸ“Š ì •ë¦¬ëœ ì»¬ëŸ¼ë“¤: {list(catalog_df.columns)}")
    
    # ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§
    # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ìˆ«ìì¸ í–‰ë§Œ ì„ íƒ
    if len(catalog_df.columns) > 0:
        first_col_name = catalog_df.columns[0]
        # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œ í–‰ë§Œ ì„ íƒ
        numeric_mask = pd.to_numeric(catalog_df[first_col_name], errors='coerce').notna()
        catalog_clean = catalog_df[numeric_mask].copy()
        
        # ì»¬ëŸ¼ëª…ì´ ì´ìƒí•˜ë©´ í‘œì¤€ ì´ë¦„ìœ¼ë¡œ ë³€ê²½
        if 'magnitude' not in catalog_clean.columns:
            column_mapping = {}
            cols = list(catalog_clean.columns)
            
            # ì˜ˆìƒ ìˆœì„œì— ë”°ë¼ ë§¤í•‘
            standard_names = ['number', 'magnitude', 'depth', 
                             'latitude', 'longitude', 'location']
            
            for i, std_name in enumerate(standard_names):
                if i < len(cols):
                    column_mapping[cols[i]] = std_name
            
            catalog_clean = catalog_clean.rename(columns=column_mapping)
            print(f"ğŸ”„ ì»¬ëŸ¼ëª… ë³€ê²½ ì™„ë£Œ: {column_mapping}")
        
        # ìœ„ë„/ê²½ë„ íŠ¹ë³„ ì²˜ë¦¬
        print("ğŸŒ ìœ„ë„/ê²½ë„ íŒŒì‹± ì¤‘...")
        
        if 'latitude' in catalog_clean.columns:
            print(f"  ğŸ“Š ìœ„ë„ ìƒ˜í”Œ: {catalog_clean['latitude'].head(3).tolist()}")
            catalog_clean['latitude'] = catalog_clean['latitude'].apply(parse_coordinate)
            valid_lat = catalog_clean['latitude'].dropna()
            print(f"  âœ… ìœ„ë„ íŒŒì‹± ì™„ë£Œ: {len(valid_lat)}ê°œ ì„±ê³µ")
            if len(valid_lat) > 0:
                print(f"      ë²”ìœ„: {valid_lat.min():.2f}Â° ~ {valid_lat.max():.2f}Â°")
        
        if 'longitude' in catalog_clean.columns:
            print(f"  ğŸ“Š ê²½ë„ ìƒ˜í”Œ: {catalog_clean['longitude'].head(3).tolist()}")
            catalog_clean['longitude'] = catalog_clean['longitude'].apply(parse_coordinate)
            valid_lon = catalog_clean['longitude'].dropna()
            print(f"  âœ… ê²½ë„ íŒŒì‹± ì™„ë£Œ: {len(valid_lon)}ê°œ ì„±ê³µ")
            if len(valid_lon) > 0:
                print(f"      ë²”ìœ„: {valid_lon.min():.2f}Â° ~ {valid_lon.max():.2f}Â°")

        # ìˆ«ì ì»¬ëŸ¼ë“¤ íƒ€ì… ë³€í™˜
        numeric_columns = ['magnitude', 'depth']
        for col in numeric_columns:
            if col in catalog_clean.columns:
                catalog_clean[col] = pd.to_numeric(catalog_clean[col], errors='coerce')
        
        # ìœ íš¨í•œ magnitudeê°€ ìˆëŠ” í–‰ë§Œ ì„ íƒ
        if 'magnitude' in catalog_clean.columns:
            valid_magnitude_mask = catalog_clean['magnitude'].notna()
            catalog = catalog_clean[valid_magnitude_mask].reset_index(drop=True)
        else:
            catalog = catalog_clean.reset_index(drop=True)
        
        print(f"âœ… ì¹´íƒˆë¡œê·¸ ì •ë¦¬ ì™„ë£Œ: {len(catalog)}ê°œ ìœ íš¨ ì´ë²¤íŠ¸")
        
        if len(catalog) > 0:
            print(f"ğŸ“Š ì¹´íƒˆë¡œê·¸ ì •ë³´:")
            print(f"  -> ì»¬ëŸ¼ë“¤: {list(catalog.columns)}")
            if 'magnitude' in catalog.columns:
                valid_mag = catalog['magnitude'].dropna()
                if len(valid_mag) > 0:
                    print(f"  -> ê·œëª¨ ë²”ìœ„: {valid_mag.min():.1f} - {valid_mag.max():.1f}")
            
            if 'latitude' in catalog.columns and 'longitude' in catalog.columns:
                valid_coords = catalog[['latitude', 'longitude']].dropna()
                if len(valid_coords) > 0:
                    print(f"  -> ìœ„ì¹˜ ë²”ìœ„:")
                    print(f"      ìœ„ë„: {valid_coords['latitude'].min():.2f}Â° ~ {valid_coords['latitude'].max():.2f}Â°")
                    print(f"      ê²½ë„: {valid_coords['longitude'].min():.2f}Â° ~ {valid_coords['longitude'].max():.2f}Â°")
            
            print(f"ğŸ“‹ ì²« 3ê°œ ì´ë²¤íŠ¸:")
            display_cols = [col for col in ['number', 'magnitude', 'depth', 'latitude', 'longitude'] 
                          if col in catalog.columns]
            if display_cols:
                first_3 = catalog[display_cols].head(3)
                print(first_3)
                
                # ì¢Œí‘œ ê°’ ìƒì„¸ í™•ì¸
                print(f"\nğŸ” ì¢Œí‘œ ìƒì„¸ ì •ë³´:")
                for i in range(min(3, len(catalog))):
                    lat = catalog.iloc[i]['latitude'] if 'latitude' in catalog.columns else None
                    lon = catalog.iloc[i]['longitude'] if 'longitude' in catalog.columns else None
                    print(f"  ì´ë²¤íŠ¸ {i+1}: ìœ„ë„={lat}, ê²½ë„={lon}")
    
    else:
        raise ValueError("ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")

except Exception as e:
    print(f"âŒ CSV ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

# ============================================================================
# 2ë‹¨ê³„: 3-channel data combination
# ============================================================================
print(f"\n   Step2: 3-channel data combination")

# Define channel order
channel_order = ['vertical_BHZ', 'horizontal_1_BH1', 'horizontal_2_BH2']
combined_channels = []

print("Channel combining progress:")
for channel_name in channel_order:
    if channel_name in final_processed_data:
        data = final_processed_data[channel_name]['data']
        combined_channels.append(data)
        print(f"  âœ… {channel_name}: Add {len(data)} samples")
    else:
        print(f"  âŒ {channel_name}: No such channel")

if len(combined_channels) == 3:
    # (ì‹œê°„, ì±„ë„) í˜•íƒœë¡œ ê²°í•©
    combined_data = np.column_stack(combined_channels)
    print(f"âœ… Completed 3-channel combining: {combined_data.shape} (Time x Channel)")
    
    # ê¸°ë³¸ í†µê³„
    print(f"  -> Data range: {combined_data.min():.3f} ~ {combined_data.max():.3f}")
    print(f"  -> Mean: {np.mean(combined_data):.3f}")
    print(f"  -> Standarad Deviation: {np.std(combined_data):.3f}")
else:
    print("âŒ 3-channel combining Failed")
    combined_data = None

# ============================================================================
# Step3: Time-based windowing
# ============================================================================
print(f"\n   Step3: Time-based windowing")

# ìœˆë„ìš° íŒŒë¼ë¯¸í„° ì„¤ì •
sampling_rate = 40  # Hz
window_duration = 20  # ì´ˆ
window_samples = int(window_duration * sampling_rate)  # 800 samples
overlap_ratio = 0.5  # 50% ê²¹ì¹¨
overlap_samples = int(window_samples * overlap_ratio)

print(f"Window setting:")
print(f"  ğŸ• Window length: {window_duration} seconds ({window_samples} samples)")
print(f"  ğŸ”„ Overlap: {overlap_ratio*100}% ({overlap_samples} samples)")

# Catalog-based windowing function
def create_earthquake_windows(combined_data, catalog, sampling_rate, 
                             window_samples, before_seconds=10, after_seconds=10):
    """Create window based on earthquake events"""
    
    windows = []
    labels = []
    metadata = []
    
    print(f"\nCreate window for earthquake events:")
    
    # Iterate over a pandas DataFrame (ìˆœíšŒ)
    for idx in range(len(catalog)):
        try:
            # Row Acess using iloc
            event = catalog.iloc[idx]
            magnitude = float(event['magnitude'])
            
            print(f"  ğŸ“ Event {idx+1}: M{magnitude}")
            
            # ì‹œê°„ ì •ë³´ (ì‹¤ì œë¡œëŠ” ì¹´íƒˆë¡œê·¸ì˜ ì‹œê°„ê³¼ ì§€ì§„íŒŒ ë°ì´í„°ì˜ ì‹œê°„ì„ ë§¤ì¹­í•´ì•¼ í•¨)
            # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ë°ì´í„° ì¤‘ì•™ ë¶€ë¶„ì„ ì§€ì§„ ë°œìƒ ì‹œì ìœ¼ë¡œ ê°€ì •
            total_samples = len(combined_data)
            earthquake_sample = total_samples // 2  # ì¤‘ì•™ì ì„ ì§€ì§„ ë°œìƒìœ¼ë¡œ ê°€ì •
            
            # ì§€ì§„ ì „í›„ êµ¬ê°„ ê³„ì‚°
            before_samples = int(before_seconds * sampling_rate)
            after_samples = int(after_seconds * sampling_rate)
            
            start_sample = earthquake_sample - before_samples
            end_sample = earthquake_sample + after_samples
            
            # ìœ íš¨ ë²”ìœ„ í™•ì¸
            if start_sample >= 0 and end_sample <= total_samples:
                event_window = combined_data[start_sample:end_sample]
                
                # ìœˆë„ìš°ê°€ ì¶©ë¶„íˆ ê¸´ì§€ í™•ì¸
                if len(event_window) >= window_samples:
                    # ì´ë²¤íŠ¸ ìœˆë„ìš° ë‚´ì—ì„œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±
                    event_window_count = 0
                    overlap_samples = window_samples // 2  # 50% ê²¹ì¹¨
                    
                    for i in range(0, len(event_window) - window_samples + 1, overlap_samples):
                        window = event_window[i:i + window_samples]
                        
                        if len(window) == window_samples:
                            windows.append(window)
                            
                            # ì•ˆì „í•˜ê²Œ ë¼ë²¨ ìƒì„±
                            label_dict = {
                                'magnitude': magnitude,
                                'depth': float(event['depth']),
                                'latitude': float(event['latitude']),
                                'longitude': float(event['longitude']),
                                'event_id': idx,
                                'window_in_event': event_window_count
                            }
                            labels.append(label_dict)
                            
                            metadata_dict = {
                                'earthquake_sample': earthquake_sample,
                                'window_start': start_sample + i,
                                'window_end': start_sample + i + window_samples,
                                'relative_to_earthquake': i - before_samples
                            }
                            metadata.append(metadata_dict)
                            
                            event_window_count += 1
                    
                    print(f"    âœ… {event_window_count} Windows created")
                else:
                    print(f"    âš ï¸ Event window too short: {len(event_window)} samples")
            else:
                print(f"    âš ï¸ Event is out of data range")
                
        except Exception as e:
            print(f"    âŒ ì´ë²¤íŠ¸ {idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    
    return np.array(windows), labels, metadata

# ì§€ì§„ ì´ë²¤íŠ¸ ê¸°ë°˜ ìœˆë„ìš° ìƒì„±
print("Generating window based on earthquake event...")

# ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ ì•ˆì „í•˜ê²Œ í™•ì¸
if combined_data is not None:
    print(f"âœ… combined_data ready: {combined_data.shape}")
else:
    print("âŒ No combined_data")

try:
    if len(catalog) > 0:
        print(f"âœ… catalog ready: {len(catalog)} events")
        catalog_ready = True
    else:
        print("âŒ Empty catalog")
        catalog_ready = False
except:
    print("âŒ There is a catalog problem")
    catalog_ready = False

# Actual window generation
if combined_data is not None and catalog_ready:
    print("Call window generation function")
    event_windows, event_labels, event_metadata = create_earthquake_windows(
        combined_data, catalog, sampling_rate, window_samples
    )
else:
    print("âš ï¸ Window generation conditions unsatisfied")
    event_windows = np.array([])
    event_labels = []
    event_metadata = []
    
    print(f"\nâœ… Completed event-based window generation:")
    print(f"  -> Total number of windows: {len(event_windows)}")
    if len(event_windows) > 0:
        print(f"  -> Window shape: {event_windows[0].shape}")
        print(f"  -> Total shape: {event_windows.shape}")

# ============================================================================
#  Step4: Create a background noise window (non-earthquake region)
# ============================================================================
print(f"\n   Step4: Create a background noise window (non-earthquake region)")

def create_background_windows(combined_data, window_samples, overlap_samples, 
                             exclude_ranges=None):
    """Create a background noise window (non-earthquake region)"""
    
    background_windows = []
    background_metadata = []
    
    # ì „ì²´ ë°ì´í„°ì—ì„œ ì§€ì§„ êµ¬ê°„ì„ ì œì™¸í•œ ë¶€ë¶„ì—ì„œ ìœˆë„ìš° ìƒì„±
    total_samples = len(combined_data)
    
    # ì§€ì§„ êµ¬ê°„ ì œì™¸ (ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì¤‘ì•™ 1/3 êµ¬ê°„ì„ ì§€ì§„ êµ¬ê°„ìœ¼ë¡œ ê°€ì •)
    exclude_start = total_samples // 3
    exclude_end = total_samples * 2 // 3
    
    print(f"Background noise range:")
    print(f"  ğŸ”¸ Range 1: 0 ~ {exclude_start} samples")
    print(f"  ğŸ”¸ Range 2: {exclude_end} ~ {total_samples} samples")
    
    # ì²« ë²ˆì§¸ êµ¬ê°„ì—ì„œ ìœˆë„ìš° ìƒì„±
    for i in range(0, exclude_start - window_samples, overlap_samples):
        window = combined_data[i:i + window_samples]
        if len(window) == window_samples:
            background_windows.append(window)
            background_metadata.append({
                'window_start': i,
                'window_end': i + window_samples,
                'type': 'background_1'
            })
    
    # ë‘ ë²ˆì§¸ êµ¬ê°„ì—ì„œ ìœˆë„ìš° ìƒì„±
    for i in range(exclude_end, total_samples - window_samples, overlap_samples):
        window = combined_data[i:i + window_samples]
        if len(window) == window_samples:
            background_windows.append(window)
            background_metadata.append({
                'window_start': i,
                'window_end': i + window_samples,
                'type': 'background_2'
            })
    
    return np.array(background_windows), background_metadata

# ë°°ê²½ ë…¸ì´ì¦ˆ ìœˆë„ìš° ìƒì„±
if combined_data is not None:
    background_windows, background_metadata = create_background_windows(
        combined_data, window_samples, overlap_samples
    )
    
    print(f"âœ… Background noise window creation complete:")
    print(f"  ğŸ“Š Number of background windows: {len(background_windows)}")
    if len(background_windows) > 0:
        print(f"  ğŸ“Š Window shape: {background_windows[0].shape}")

# ============================================================================
# Step5: Create data pairs for noise removal
# ============================================================================
print(f"\n   Step5: Create data pairs for noise removal")

def add_realistic_noise(clean_windows, noise_level=0.1):
    """Add realistic noise to clean signal"""
    
    noisy_windows = []
    
    print(f"Add noise in progress:")
    print(f"  ğŸ”Š Nosie level: {noise_level}")
    
    for i, clean_window in enumerate(clean_windows):
        # 1. Gaussian noise
        gaussian_noise = np.random.normal(0, noise_level * np.std(clean_window), 
                                        clean_window.shape)
        
        # 2. Power line interference (ì „ë ¥ì„  ê°„ì„­) (60Hz)
        time_axis = np.arange(len(clean_window)) / sampling_rate
        power_line_noise = 0.05 * noise_level * np.sin(2 * np.pi * 60 * time_axis)
        
        # 3. Low Frequency Drift
        drift_noise = 0.02 * noise_level * np.sin(2 * np.pi * 0.1 * time_axis)
        
        # Apply different noise characteristics to each channel 
        noisy_window = clean_window.copy()
        for ch in range(clean_window.shape[1]):
            channel_noise = (gaussian_noise[:, ch] + 
                           power_line_noise * (0.5 + 0.5 * ch) +  # Different intensity for each channel
                           drift_noise[:, ch] if len(drift_noise.shape) > 1 
                           else np.broadcast_to(drift_noise, (len(drift_noise),)))
            noisy_window[:, ch] += channel_noise
        
        noisy_windows.append(noisy_window)
        
        if (i + 1) % 10 == 0 or i == len(clean_windows) - 1:
            print(f"    Progress: {i+1}/{len(clean_windows)} ({(i+1)/len(clean_windows)*100:.1f}%)")
    
    return np.array(noisy_windows)

# Use Event Window with clean data
if 'event_windows' in locals() and len(event_windows) > 0:
    clean_data = event_windows
    noisy_data = add_realistic_noise(clean_data, noise_level=0.15)
    
    print(f"âœ… Data pair creation for noise removal completed:")
    print(f"  ğŸ“Š Clean Data: {clean_data.shape}")
    print(f"  ğŸ“Š Noisy Data: {noisy_data.shape}")
    
    # Check the effect of adding noise
    print(f"  -> Effect of adding noise:")
    print(f"    Clean std: {np.std(clean_data):.4f}")
    print(f"    Noisy std: {np.std(noisy_data):.4f}")
    print(f"    Noise ratio: {(np.std(noisy_data) - np.std(clean_data))/np.std(clean_data)*100:.1f}%")

# ============================================================================
# Step6: Final normalization and data construction
# ============================================================================
print(f"\n   Step6: Final normalization and data construction")

def normalize_data(data, method='z_score'):
    """ë°ì´í„° ì •ê·œí™”"""
    if method == 'z_score':
        mean = np.mean(data)
        std = np.std(data)
        normalized = (data - mean) / (std + 1e-8)  # Prevent division by zero
    elif method == 'min_max':
        min_val = np.min(data)
        max_val = np.max(data)
        normalized = (data - min_val) / (max_val - min_val + 1e-8)
    
    return normalized, {'mean': mean if method == 'z_score' else min_val,
                       'scale': std if method == 'z_score' else (max_val - min_val)}

# Normalize data
if 'noisy_data' in locals() and 'clean_data' in locals():
    print("Proceed data normalization:")
    
    # Compute statistics for the entire dataset
    all_data = np.concatenate([noisy_data.flatten(), clean_data.flatten()])
    
    # Z-score Normalization
    normalized_noisy, norm_stats = normalize_data(noisy_data, method='z_score')
    normalized_clean, _ = normalize_data(clean_data, method='z_score')
    
    print(f"  âœ… Completed Z-score Normalization")
    print(f"    Mean: {norm_stats['mean']:.6f}")
    print(f"    Standard deviation: {norm_stats['scale']:.6f}")
    print(f"    Range after normalization: {normalized_noisy.min():.3f} ~ {normalized_noisy.max():.3f}")

# ============================================================================
# Step7: Split train/validation data
# ============================================================================
print(f"\n   Step7: Split train/validation data")

def split_dataset(X, y, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):
    """Split train/validation data"""
    
    total_samples = len(X)
    print(f"  ğŸ“Š Total samples: {total_samples}")
    
    # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ë³´ì¥
    if total_samples < 3:
        print(f"  âš ï¸ Too few samples ({total_samples}) for proper split")
        print(f"  ğŸ”§ Applying emergency split strategy")
        
        if total_samples == 1:
            # 1ê°œë¿ì´ë©´ ëª¨ë‘ trainì—
            return {
                'train': {'X': X, 'y': y, 'indices': np.array([0])},
                'val': {'X': X[:0], 'y': y[:0], 'indices': np.array([])},
                'test': {'X': X[:0], 'y': y[:0], 'indices': np.array([])}
            }
        elif total_samples == 2:
            # 2ê°œë©´ train 1ê°œ, val 1ê°œ, test 0ê°œ
            indices = np.random.permutation(total_samples)
            return {
                'train': {'X': X[indices[:1]], 'y': y[indices[:1]], 'indices': indices[:1]},
                'val': {'X': X[indices[1:2]], 'y': y[indices[1:2]], 'indices': indices[1:2]},
                'test': {'X': X[:0], 'y': y[:0], 'indices': np.array([])}
            }
    
    # ì •ìƒì ì¸ ê²½ìš° (3ê°œ ì´ìƒ)
    indices = np.random.permutation(total_samples)
    
    # ìµœì†Œ 1ê°œì”© ë³´ì¥í•˜ë©´ì„œ ë¶„í• 
    min_val_samples = max(1, int(total_samples * val_ratio))
    min_test_samples = max(1, int(total_samples * test_ratio))
    min_train_samples = total_samples - min_val_samples - min_test_samples
    
    # ìŒìˆ˜ê°€ ë˜ëŠ” ê²½ìš° ì¡°ì •
    if min_train_samples < 1:
        min_train_samples = 1
        min_val_samples = (total_samples - 1) // 2
        min_test_samples = total_samples - min_train_samples - min_val_samples
    
    # ë¶„í•  ì§€ì  ê³„ì‚°
    train_end = min_train_samples
    val_end = train_end + min_val_samples
    
    # Split
    train_idx = indices[:train_end]
    val_idx = indices[train_end:val_end]
    test_idx = indices[val_end:]
    
    return {
        'train': {'X': X[train_idx], 'y': y[train_idx], 'indices': train_idx},
        'val': {'X': X[val_idx], 'y': y[val_idx], 'indices': val_idx},
        'test': {'X': X[test_idx], 'y': y[test_idx], 'indices': test_idx}
    }

# Split data
if 'normalized_noisy' in locals() and 'normalized_clean' in locals():
    # ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ë¶„í• 
    np.random.seed(42)
    
    dataset = split_dataset(normalized_noisy, normalized_clean, 
                          train_ratio=0.7, val_ratio=0.2, test_ratio=0.1)
    
    print(f"âœ… Data Split Completed:")
    for split_name, split_data in dataset.items():
        print(f"  ğŸ“Š {split_name.upper()}: {len(split_data['X'])} samples")
        print(f"    X shape: {split_data['X'].shape}")
        print(f"    y shape: {split_data['y'].shape}")

# ============================================================================
# Summary of final results
# ============================================================================
print(f"\n Summary of final results")
print("="*60)

if 'dataset' in locals():
    print(f"ğŸ“Š Final Dataset:")
    print(f"  ğŸ¯ Progress: Seismic wave Denoising")
    print(f"  ğŸ“ Input order: {dataset['train']['X'].shape[1:]} (ì‹œê°„ x ì±„ë„)")
    print(f"  ğŸ“ˆ Data Normalization: Z-score")
    print(f"  ğŸ”€ Data Split:")
    for split_name, split_data in dataset.items():
        ratio = len(split_data['X']) / (len(dataset['train']['X']) + 
                                      len(dataset['val']['X']) + 
                                      len(dataset['test']['X'])) * 100
        print(f"    - {split_name.upper()}: {len(split_data['X'])}ê°œ ({ratio:.1f}%)")
    # print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ!")
    # print(f"  ğŸ’¡ ì¶”ì²œ ëª¨ë¸: U-Net, Autoencoder, or Transformer-based denoiser")
    # print(f"  ğŸ“ ì‚¬ìš©ë²•:")
    # print(f"    X_train = dataset['train']['X']")
    # print(f"    y_train = dataset['train']['y']")

else:
    print(f"âŒ Some steps Failed - Debugging required")

print(f"\nâœ… Deeplearning preprocessing completed!")




import pandas as pd
import numpy as np

print("Result data file saving starts!")
print("="*60)

def save_earthquake_data_to_csv(dataset):
    """ë”¥ëŸ¬ë‹ ì „ì²˜ë¦¬ëœ ì§€ì§„íŒŒ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥"""
    
    def reshape_and_save(data_X, data_y, filename_prefix):
        """3D ì§€ì§„íŒŒ ë°ì´í„°ë¥¼ 2D CSVë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥"""
        
        print(f"ğŸ’¾ {filename_prefix} ì„¸íŠ¸ ì €ì¥ ì¤‘...")
        
        n_samples, n_time, n_channels = data_X.shape
        print(f"  ğŸ“Š í˜•íƒœ: {n_samples}ê°œ ìƒ˜í”Œ Ã— {n_time}ì‹œì  Ã— {n_channels}ì±„ë„")
        
        # === X ë°ì´í„° (ë…¸ì´ì¦ˆ ìˆëŠ” ë°ì´í„°) ì €ì¥ ===
        # (1664, 800, 3) â†’ (1664, 2400) í˜•íƒœë¡œ ë³€í™˜
        X_reshaped = data_X.reshape(n_samples, -1)
        
        # ì»¬ëŸ¼ëª… ìƒì„±: t0_ch0, t0_ch1, t0_ch2, t1_ch0, t1_ch1, t1_ch2, ...
        X_columns = []
        for t in range(n_time):
            for ch in range(n_channels):
                channel_name = ['BHZ', 'BH1', 'BH2'][ch]  # ì‹¤ì œ ì±„ë„ëª… ì‚¬ìš©
                X_columns.append(f't{t}_{channel_name}')
        
        # DataFrame ìƒì„± ë° ì €ì¥
        X_df = pd.DataFrame(X_reshaped, columns=X_columns)
        X_df.to_csv(f'{filename_prefix}_X_noisy.csv', index=False)
        print(f"  âœ… {filename_prefix}_X_noisy.csv ì €ì¥ì™„ë£Œ ({X_df.shape})")
        
        # === y ë°ì´í„° (ê¹¨ë—í•œ ë°ì´í„°) ì €ì¥ ===
        y_reshaped = data_y.reshape(n_samples, -1)
        
        # ê°™ì€ ì»¬ëŸ¼ëª… ì‚¬ìš©
        y_columns = X_columns  # ë™ì¼í•œ êµ¬ì¡°
        
        y_df = pd.DataFrame(y_reshaped, columns=y_columns)
        y_df.to_csv(f'{filename_prefix}_y_clean.csv', index=False)
        print(f"  âœ… {filename_prefix}_y_clean.csv ì €ì¥ì™„ë£Œ ({y_df.shape})")
        
        return X_df.shape, y_df.shape
    
    # ê° ë°ì´í„°ì…‹ ì €ì¥
    total_saved = 0
    
    for split_name in ['train', 'val', 'test']:
        if split_name in dataset and len(dataset[split_name]['X']) > 0:
            X_shape, y_shape = reshape_and_save(
                dataset[split_name]['X'], 
                dataset[split_name]['y'], 
                split_name
            )
            total_saved += X_shape[0]
        else:
            print(f"âš ï¸ {split_name} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    return total_saved

# ë©”íƒ€ë°ì´í„° ì €ì¥
def save_metadata_csv(dataset):
    """ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥"""
    
    print(f"\nğŸ“Š ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘...")
    
    # ê¸°ë³¸ ì •ë³´
    metadata = []
    
    for split_name in ['train', 'val', 'test']:
        if split_name in dataset:
            split_data = dataset[split_name]
            metadata.append({
                'split': split_name,
                'samples': len(split_data['X']),
                'time_steps': split_data['X'].shape[1] if len(split_data['X']) > 0 else 0,
                'channels': split_data['X'].shape[2] if len(split_data['X']) > 0 else 0,
                'total_features': split_data['X'].shape[1] * split_data['X'].shape[2] if len(split_data['X']) > 0 else 0
            })
    
    # ì „ì²´ ì •ë³´ ì¶”ê°€
    total_samples = sum(len(dataset[split]['X']) for split in ['train', 'val', 'test'] if split in dataset)
    
    metadata.append({
        'split': 'TOTAL',
        'samples': total_samples,
        'time_steps': 800,
        'channels': 3,
        'total_features': 2400
    })
    
    # ì„¤ì • ì •ë³´ ì¶”ê°€
    settings = pd.DataFrame([
        {'parameter': 'sampling_rate', 'value': '40 Hz'},
        {'parameter': 'window_duration', 'value': '20 seconds'},
        {'parameter': 'normalization', 'value': 'Z-score'},
        {'parameter': 'noise_level', 'value': '15%'},
        {'parameter': 'original_events', 'value': '1664'},
        {'parameter': 'channels', 'value': 'BHZ, BH1, BH2'},
        {'parameter': 'data_split', 'value': '70% train, 20% val, 10% test'}
    ])
    
    # ì €ì¥
    metadata_df = pd.DataFrame(metadata)
    metadata_df.to_csv('dataset_metadata.csv', index=False)
    settings.to_csv('dataset_settings.csv', index=False)
    
    print(f"  âœ… dataset_metadata.csv ì €ì¥ì™„ë£Œ")
    print(f"  âœ… dataset_settings.csv ì €ì¥ì™„ë£Œ")
    
    return metadata_df

# ì‹¤í–‰
if 'dataset' in locals():
    print(f"ğŸ¯ í˜„ì¬ ë°ì´í„°ì…‹ ìƒíƒœ:")
    for split_name in ['train', 'val', 'test']:
        if split_name in dataset:
            print(f"  ğŸ“š {split_name}: {len(dataset[split_name]['X'])}ê°œ ìƒ˜í”Œ")
    
    # CSV ì €ì¥ ì‹¤í–‰
    total_saved = save_earthquake_data_to_csv(dataset)
    metadata_df = save_metadata_csv(dataset)
    """""
    print(f"\nğŸ‰ CSV ì €ì¥ ì™„ë£Œ!")
    print(f"  ğŸ“Š ì´ ì €ì¥ëœ ìƒ˜í”Œ: {total_saved}ê°œ")
    print(f"  ğŸ“„ ìƒì„±ëœ íŒŒì¼ë“¤:")
    print(f"    - train_X_noisy.csv (ë…¸ì´ì¦ˆ ìˆëŠ” í›ˆë ¨ ë°ì´í„°)")
    print(f"    - train_y_clean.csv (ê¹¨ë—í•œ í›ˆë ¨ ë°ì´í„°)")
    print(f"    - val_X_noisy.csv (ë…¸ì´ì¦ˆ ìˆëŠ” ê²€ì¦ ë°ì´í„°)")
    print(f"    - val_y_clean.csv (ê¹¨ë—í•œ ê²€ì¦ ë°ì´í„°)")
    print(f"    - test_X_noisy.csv (ë…¸ì´ì¦ˆ ìˆëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°)")
    print(f"    - test_y_clean.csv (ê¹¨ë—í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°)")
    print(f"    - dataset_metadata.csv (ë°ì´í„°ì…‹ ì •ë³´)")
    print(f"    - dataset_settings.csv (ì„¤ì • ì •ë³´)")
    """""
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    import os
    print(f"\nğŸ“ íŒŒì¼ í¬ê¸°:")
    csv_files = [
        'train_X_noisy.csv', 'train_y_clean.csv',
        'val_X_noisy.csv', 'val_y_clean.csv', 
        'test_X_noisy.csv', 'test_y_clean.csv'
    ]
    
    for file in csv_files:
        if os.path.exists(file):
            size_mb = os.path.getsize(file) / 1024 / 1024
            print(f"  ğŸ“„ {file}: {size_mb:.1f} MB")
    
    print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
    print(f"  # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°")
    print(f"  train_X = pd.read_csv('train_X_noisy.csv')")
    print(f"  train_y = pd.read_csv('train_y_clean.csv')")
    print(f"  # ë”¥ëŸ¬ë‹ í•™ìŠµì— ì‚¬ìš©!")
    
else:
    print("âŒ 'dataset' ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ ë¨¼ì € ë”¥ëŸ¬ë‹ ì „ì²˜ë¦¬ë¥¼ ì™„ë£Œí•´ì£¼ì„¸ìš”.")


## ë”¥ëŸ¬ë‹ìš©ìœ¼ë¡œ ë³€í™˜
import pandas as pd
import numpy as np

print("ğŸš€ CSV â†’ ë”¥ëŸ¬ë‹ ë°ì´í„° ë³€í™˜ ì‹œì‘!")

def csv_to_deeplearning_ready(csv_prefix_list=['train', 'val', 'test']):
    """CSVì—ì„œ ë”¥ëŸ¬ë‹ ì¤€ë¹„ ì™„ë£Œ ë°ì´í„°ë¡œ í•œ ë²ˆì— ë³€í™˜"""
    
    dataset = {}
    
    for prefix in csv_prefix_list:
        print(f"ğŸ”„ {prefix} ë°ì´í„° ë¡œë”© ì¤‘...")
        
        try:
            # CSV íŒŒì¼ ì½ê¸°
            X_file = f'{prefix}_X_noisy.csv'
            y_file = f'{prefix}_y_clean.csv'
            
            X_df = pd.read_csv(X_file)
            y_df = pd.read_csv(y_file)
            
            # 3D ë³€í™˜: (ìƒ˜í”Œ, 2400) â†’ (ìƒ˜í”Œ, 800, 3)
            n_samples = len(X_df)
            X_3d = X_df.values.reshape(n_samples, 800, 3)
            y_3d = y_df.values.reshape(n_samples, 800, 3)
            
            # ë”•ì…”ë„ˆë¦¬ì— ì €ì¥
            dataset[prefix] = {
                'X': X_3d,  # ë…¸ì´ì¦ˆ ìˆëŠ” ë°ì´í„°
                'y': y_3d   # ê¹¨ë—í•œ ë°ì´í„°
            }
            
            print(f"  âœ… {prefix}: {X_3d.shape} â†’ {y_3d.shape}")
            
        except FileNotFoundError:
            print(f"  âŒ {prefix} íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"  âŒ {prefix} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return dataset

# ì‹¤í–‰!
dl_dataset = csv_to_deeplearning_ready()

# ê²°ê³¼ í™•ì¸
if dl_dataset:
    print(f"\nğŸ‰ ë³€í™˜ ì™„ë£Œ!")
    for split_name, data in dl_dataset.items():
        print(f"  ğŸ“Š {split_name}: {data['X'].shape} (ë…¸ì´ì¦ˆ) â†’ {data['y'].shape} (ê¹¨ë—í•¨)")
    
    print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
    print(f"  train_X = dl_dataset['train']['X']")
    print(f"  train_y = dl_dataset['train']['y']")
    print(f"  # ì´ì œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥! ğŸš€")
else:
    print("âŒ ë³€í™˜ ì‹¤íŒ¨")
